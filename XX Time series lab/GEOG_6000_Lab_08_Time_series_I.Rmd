---
title: "GEOG 6000 Lab 08 Time series Lab"
author: "Simon Brewer"
date: "October 12, 2019"
output: 
  pdf_document: 
    number_sections: yes
header-includes:
   - \usepackage{tabularx}
---

```{r echo=FALSE}
options(width=50)
set.seed(1234)
```

In this lab, we will at how to create time series objects in R, how to manipulate them to make them stationary, how to describe the autocorrelation in the series, and finally how to build an ARIMA model. You will need to install the **forecast** package to run all the code in this lab. We will be using three file for these examples:

- The Mauna Loa CO$_2$ dataset: *co2_mm_mlo.csv*
- A dataset of US GNP from 1947 to 2002: *gnp.csv*
- A dataset of cigarette consumption in Canada: *CanadaCigaretteConsumption.csv*

Things to remember:

- The following font conventions are used:
    - Normal font = text, comments etc
    - `Courier font` = R commands
    - *Italic font* = file names
- R is case-sensitive for variables and functions. Iris is not the same as iris or IRIS.
- `#` indicates comments - no need to type these
- **Remember to change your R working directory to where your files are stored**

**With all the examples given, it is important to not just type in the code, but to try changing the parameters and re-running the functions multiple times to get an idea of their effect.** Help on the parameters can be obtained by typing `help(functionname)` or `?functionname`. 

\newpage

# Time Series Objects
Read in the CO$_2$ data and GNP data as follows:

```{r}
co2 <- read.csv('co2_mm_mlo.csv')
gnp <- read.csv('gnp.csv')
head(co2)
```

While the `co2` data frame contains variables with date information, R will not recognize it as such unless it is explicitly told so. If we look at the start of the dataframe, we can see that the record starts in March 1958, and we can convert the CO$_2$ data into a time series *object* as follows:

```{r}
co2.ts <- ts(co2$CO2int, start=c(1958,3), freq=12)
```

This uses the `ts()` function to take a column of numbers from the data frame, and convert it into a time series object by attaching an explicit time scale. This is done by the parameters `start`, which gives the time of the first observation , and `freq`, which gives the periodicity of the time steps (in this case 12 months).

Now convert the quarterly GNP data into a time series object:
```{r}
head(gnp)
gnp.ts <- ts(gnp$GNP, start=c(1947,1), freq=4)
```

Now plot the time series object --- note that R automatically plots the values against the time scale you have defined.
```{r}
plot(co2.ts)
plot(gnp.ts)
```

Some other useful functions with time series objects are `time()`, which extracts a vector of the time points and `window()`, which allows you to extract a subset of points as a new time series. This requires a start and end parameter --- the following example extracts only the data for 2006-2016

```{r warning=FALSE, fig.keep='none'}
co2.ts.window <- window(co2.ts, start=c(2006,1), end=c(2016,12))
plot(co2.ts.window)
```

# Dealing with non-stationarity

Time series analysis assume that the series are stationary, i.e. the mean or variance do not change. However, many series do exhibit some form of changes, the most common of which is the type of long-term trend seen in both the GNP and CO2 data. There are several ways to remove these trends, and we will look briefly at some of these here:

As the first two methods of trend removal require model fitting, we need to extract a vector of time points and a vector of values for use in the modeling functions. We will use the GNP data as an example:

```{r results='hide'}
t <- as.vector(time(gnp.ts))
gnp.x <- as.numeric(gnp.ts)
```

## Linear trends
The simplest way to remove a trend is by fitting a linear model to it, then subtract the predicted value at each data point. We use the `lm()` function to fit a linear model by OLS. Use the `summary()` function to see some model diagnostics, and the `fitted()` function to extract predicted values for each datapoint.

```{r results='hide'}
gnp.lm <- lm(gnp.x ~ t)
summary(gnp.lm)
gnp.trend <- fitted(gnp.lm)
```

Now plot the model and the detrended series to see the effect of removing the linear trend.
We use the `par()` function to split the plot into two panels and the
`lines()` function to add extra lines to the plots.
```{r fig.keep='none',results='hide'}
par(mfrow=c(2,1))
plot(t, gnp.x, main='US GNP', type='l')
lines(t, gnp.trend, col='red')
plot(t, gnp.x - gnp.trend, type='l', main="Linear Detrended")
abline(h=0, lty=2)
```

## Local smoothers
The detrended series following removal of the linear trend still shows show non-stationarity, notably at the ends of the series. A locally fitted smoother, such as a loess smoother will improve the fit to the non-linear part of the trend.
```{r fig.keep='none',results='hide'}
gnp.loess <- loess(gnp.x ~ t)
gnp.trend <- gnp.loess$fitted
par(mfrow=c(2,1))
plot(t, gnp.x, main='US GNP', type='l')
lines(t, gnp.trend, col='red')
plot(t, gnp.x - gnp.trend, type='l', main="Loess Detrended")
abline(h=0, lty=2)
```

The `loess()` function takes a parameter `span` which controls the size of the window for the local smoother. By default this is 0.75, or 75% of the time window. Try to repeat this with a smaller window size.

## Differencing
Differencing is a further way to remove trends, by converting a time series into a series of values that are simply the difference or amount of change between time points. Here we use the `diff()` function that calculates $X_t - X_{t-1}$.
```{r fig.keep='none',results='hide'}
gnp.diff.1 <- diff(gnp.ts)
gnp.diff.2 <- diff(gnp.ts, differences = 2)
par(mfrow=c(2,1))
plot(gnp.diff.1, main="First Order Difference", lwd=2)
plot(gnp.diff.2, main="Second Order Difference", lwd=2)
```
The first order difference effectively removes the trend, however the resulting series shows some bias with a greater magnitude of difference in the winter months. Taking the second-order difference effectively removes this. 

## Filtering
Filtering uses a moving window to smooth out a series, and may be used for trend removal. Filters are designed as a series of weights, one for each point in a time window, that sum to one. This window is usually symmetrical and has an odd number of points (the central point + 2x the half window size). For example, the weights for a half-window of size 2 are: $[1/5,1/5,1/5,1/5,1/5]$. These can be easily created using the \texttt{rep()} function, and an example is given below using a half window of size=3 (total size $= 3 + 3 + 1 = 7$).
```{r fig.keep='none',results='hide'}
gnp.flt <- filter(gnp.ts,filter=rep(1/7,7))
par(mfrow=c(2,1))
plot(gnp.ts, main='US GNP', lwd=2)
lines(gnp.flt,col="red", lwd=2)
plot(gnp.ts-gnp.flt)
```
Try a second filter with a larger half window (e.g. 6 or 12). Note that as the size of the filter increases, more of the small scale variability is removed.

# Removing seasonality
The CO$_2$ series has a very strong seasonal cycle due to variations in the photosynthetic activity of plants at high latitudes, and this will dominate any attempt to characterize or model the time series. The seasonal cycle can be removed by either:

- Calculating the long-term mean of each month and subtracting that from the individual monthly values. This assumes that the standard seasonal cycle remains constant through time
- Calculating a smoothed series of each month's values and subtracting that. This method allows the seasonal cycle to vary (become stronger or weaker) over time. 

We will look at the second of these methods here, with the function `stl()`. This decomposes seasonal time series data into three parts: a) a long-term trend; b) a seasonal cycle; c) remaining autocorrelated time series. It does this by fitting a loess model to the entire series to identify the long-term trend. A detrended time series is then created by subtracting the loess model from the original data. It then fits individual loess models series of residual monthly values (i.e. one for January, one for February, etc). These are then aggreagted to make the series of seasonal variations in the second panel. These are then subtracted from the detrended series to leave the remaining stationary time series. The parameter `s.window` controls the size of the smoothing window for the seasonal smoother. 

```{r fig.keep='none'}
co2.ts.decompose <- stl(co2.ts, s.window=31)
plot(co2.ts.decompose)
```

The `stl()` function outputs a set of results, including a data frame with the components (seasonal, trend, remainder) from the decomposition. If we plot the seasonal cycle alone, you should be able to see that the cycle increases in magnitude over time.

```{r fig.keep='none'}
plot(co2.ts.decompose$time.series[,"seasonal"])
abline(h = seq(-3,3), lty = 2, col="lightgray")
```
If you prefer the first method for removing a seasonal cycle using long term means, the function `decompose()` will do that for you. 

# Exploring Autocorrelation

The key characteristic of time series data is that is contains 'serial' correlation or correlation between observations that are close together in time. As with classical statistics, we can explore this by looking at correlation. As we have only a single series, correlation is made within the time series by using an autocorrelation function (ACF) to compare observations over a series of time lags, i.e. between values at time $t$ and values at $t-1$, $t-2$, etc.

In R, the function `acf()` calculates the autocorrelation over a series of lags. By default, it will plot out the results with the x-axis showing the lag, and the y-axis showing the degree of correlation (-1 to 1). R calculates and plots a 95% confidence interval for the correlation (shown in thin blue lines). Correlations outside this level can be taken to be significant. We will use an improved version of this `Acf()` from the **forecast** package (you will need to install this).

Calculate and plot the ACF for the original raw GNP series. Note the significant correlation over all lags, due to the presence of a trend and seasonality.
```{r fig.keep='high'}
library(forecast)
Acf(gnp.ts, lag.max=40)
```

Now do the same with the differenced series (2nd order differences), which shows significant negative autocorrelation over the first lag. 

```{r}
Acf(gnp.diff.2)
```


Now do the same, but use the Calculate and plot the ACF for the CO$_2$ series following decomposition. The output from the `stl()` function includes the detrended and deseasonalized series, called the `remainder`. 

```{r fig.keep='high',results='hide'}
co2.ts.decompose$time.series[,"remainder"]
co2.ts.acf <- acf(co2.ts.decompose$time.series[,"remainder"], lag.max=40)
```
The ACF shows no evidence of trend or seasonality. There is a significant positive correlation at lag 1, suggesting that any two months have correlated values of CO$_2$, but after this the correlation drops to close to zero. There is some evidence for negative correlation at lags 3-7.

Autocorrelation may also be explored using the *partial* autocorrelation function (PACF). This removes the correlations that can be propagated between more distant lags due to high correlation between closer lags. This uses a regression between the intermediate lag and th high lag to capture any correlation between them, then looks for correlations between the residuals of this regression and the unlagged series. In R, this is done using the `Pacf()` function, and is often used in parallel with the `Acf()` function. For the differenced GNP data, this shows some evidence for autocorrelation over 2-3 lags
```{r fig.keep='none'}
Pacf(gnp.diff.2)
```

Note that **forecast** has a function that will plot both the ACF and PACF together with the original time series on a single figure:
```{r fig.keep='none'}
tsdisplay(gnp.diff.2)
```

# Univariate Time Series Models

## Automatic ARIMA Fitting
As choosing the various parameters for an ARIMA model can be time-consuming, methods exist to use the AIC to test which combination of AR, MA and differencing provides the best fitting model. As with all automatic fitting techniques, the results should be treated with some caution and verified as far as possible.

The **forecast** package provides a function, `auto.arima()`, which performs automatic parameter selection. Try running this with the GNP dataset:
```{r eval=FALSE}
require(forecast)
gnp.arima <- auto.arima(gnp.ts, trace=TRUE)
```

With the `trace` parameter set to TRUE, this prints every model to the screen as it is tested. On the left hand is the model in the following format: ARIMA(p,d,q)(P,D,Q)[s]. In the right-hand column  is the AIC of that model. The model with the lowest AIC is returned at the end of the selection process, and may then be used for diagnosis and prediction. 

- Try running this with the CO$_2$ time series

## ARIMA Models
ARIMA models are a flexible approach for modeling the autocorrelation of a time series, and using this to predict what the possible values of the time series will be in the future. The models are comprised of three parts:

- The autoregressive AR(p) model: this models the relationship between a variable at time $t$ and previous time lags ($p$ defines how many lags to include)
- The moving average MA(q) model: this models the relationship between a variable at time $t$ and the model errors in previous time lags ($q$ defines how many lags to include)
- The integrated (I) model: this defines how many times ($d$) the original series must be differenced to obtain a stationary series

The basic approach requires you to identify the order of these processes (e.g. $p=2$ for a second order autoregressive process). For most time series, identifying these is quite complex and requires careful inspection of the ACF and PACF. An easier way is to use an iterative method that will build a series of models for different values of $p$, $i$ and $q$, and return the best fitting model. These can be fit using the function `auto.arima()` from the **forecast** package. For the GNP data, we can run this as follows:

```{r results='hide'}
gnp.arima = auto.arima(gnp.ts, max.p = 3, max.q = 3, 
                       trace = TRUE, seasonal = FALSE)
```

Note that we set an upper limit for both $p$ and $q$, and set `seasonal` to FALSE so that this does not try to fit a seasonal model. We also set `trace` as TRUE inorder to follow the iterations. 

This identifies the best fitting model as an ARIMA(2,2,1), i.e. AR(2), MA(1) plus second order differencing. The output of the function, and we can now use this to look at this model:

```{r results='hide', fig.keep='none'}
gnp.arima
```

This shows the estimated autoregressive (`ar1` and `ar2`) and moving average (`ma1`) coefficients, together with standard errors and an AIC score. We can use the `tsdiag()` function to make diagnostic plots from the model, including the residuals, the ACF of the residuals and a test for autocorrelation over different lags. We use these to look for potential biases or remaining autocorrelation in the residuals. Remember that any regression model relies on the residuals being uncorrelated, random noise. 

```{r results='hide', fig.keep='none'}
tsdiag(gnp.arima)
```

An alternative is to simply test for autocorrelation in the residuals from the model using the Ljung-Box portmanteau test. You can do this by simply combining the two functions `residuals()` and `Box.test()`. The alternative hypothesis is that the values are independent, so a high $p$-value means that there is no evidence for autocorrelation. 
```{r results='hide', fig.keep='none'}
Box.test(residuals(gnp.arima), type = 'Ljung')
```

Now use the model to predict the next two years of GNP. The function `forecast()` allows us to do this. The parameter (`h`) gives the number of time steps over which to make predictions, in this case 20 (i.e. 5 years).
```{r results='hide', fig.keep='high'}
gnp.pred <- forecast(gnp.arima, h=20)
print(gnp.pred)
plot(gnp.pred)
```

## SARIMA Models

We can use the same approach to fit a SARIMA model to the CO$_2$ dataset. This builds from the ARIMA model by including a seasonal component. This can include a) a seasonal autoregessive process (SAR(P)); b) a seasonal moving average process (SMA(Q)); and c) seasonal differencing (i.e. the difference between values separated by one or more seasonal cycles). 

We can again use `auto.arima()` to estimate the orders for the ARIMA model and the seasonal part:

```{r results='hide'}
co2.sarima = auto.arima(co2.ts, max.p = 3, max.q = 3, 
                       trace = TRUE, seasonal = TRUE)
```


The final model that is identified is a SARIMA(1,1,1)(2,1,2)[12], which contains both short term and seasonal autoregression. Note that this also uses first order differencing for the long-term trend (this is the $d$ order) as well as long-term trends in the seasonal cycle (this is the $D$ order).

The full model coefficients can be see by inspecting the output: 
 
```{r results='markup', fig.keep='none', warning=FALSE}
co2.sarima
```

Plot the residuals and diagnostics plots as before. Are there any trends or biases in the model residuals?

Finally make a prediction using the SARIMA model. As before, we use the `forecast()` function. and predict for the following 10 years (120 time steps). 
```{r results='hide', fig.keep='none'}
co2.pred <- forecast(co2.sarima, h=120)
plot(co2.pred)
```

# ARIMAX Models

ARIMAX models allow extend the basic ARIMA model to include independent variables. We'll build a simple model using the Canadian cigarette consumption data. Start by reading the file and converting it to a time series object. Note that as there are three variable, this creates a multiple time series object.

```{r}
cig = read.csv("CanadaCigaretteConsumption.csv",
               col.names = c("cons","price","inc"))
cig.ts = ts(cig, start=1953, freq=1)
plot(cig.ts, main="Cigarette consumption")
```

We want to model consumption as a function of the other variables (income and price), so we'll start by estimating the orders for the ARIMA part of the model using `auto.arima()` and the consumption data. Note that we include the independent variables as a matrix in the model. We set the parameter `d` to one, to force the iterations to use first order differenced data. 

```{r}
regmat = data.frame(inc=cig.ts[,"inc"], price=cig.ts[,"price"])
cig.arimax = auto.arima(cig.ts[,"cons"], xreg = as.matrix(regmat),
                        max.p = 3, max.q = 3, trace = TRUE, d = 1)
```

Which suggests a ARIMA(2,0,1) model, with the following coefficients. Note that this includes both autoregressive coefficients (for the consumption data) and standard regression coefficients for price and income.

```{r}
cig.arimax
```

Make the usual checks of the residuals for autocorrelation. 

Finally, use the model to predict consumption for the next time step, using a scenario of price = 4.58 and income = 8.6
```{r}
newxreg = as.matrix(cbind(inc=4.1, price=8.8))
forecast(cig.arimax, h=1, xreg=as.matrix(newxreg))
```

Note that if you want to predict over several time steps in the future, you will need to provide independent variables for each time step.

# Exercises

1. The file *austres.csv* contains a time series of quarterly estimates of Australian resident population size for the period of 1975 to 1993. Code to read this in and convert it to a time series object is given below
    + Plot the time series. The series has a marked increasing trend with time. Plot the first order difference of the time series to remove this trend. Has this adequately removed any trend in the data? If not, make and plot the second order difference
    + Make and plot the autocorrelation and partial autocorrelation functions of the final differenced time series. Describe these plots and any autocorrelation in the series 
    + Use the `auto.arima()` function from the **forecast** package to estimate the orders of an ARIMA model for this data, and report the results of this process
    + Having obtained these orders, now build an ARIMA model using the function `Arima()`.  Report the model coefficients and the model AIC
    + A good ARIMA model will remove all autocorrelation in the model residuals. Plot these using a combination of the `Acf()` and `residuals()` functions. Use the Ljung-Box test to test for autocorrelation (`Box.test()`). Is there any remaining autocorrelation in the residuals?
    + Use the `forecast()` function to estimate the resident population for the next two years. Plot out these estimates and give the expected population size by the start (Q1) of 1995 and the 95% confidence interval of this estimate

```{r eval=FALSE}
austres <- read.csv("austres.csv")
austres.ts <- ts(austres$Residents, start=c(1971,2), freq=4)
plot(austres.ts)
```

## File details
### Mauna Loa CO$_2$ values: *co2_mm_mlo.csv*

\begin{tabularx}{\linewidth}{| l | X |}
     \hline
  	Column header & Variable \\ 
		\hline
		Year & Year \\ 
		Month & Month \\ 
  	DecDate & Year-Month as decimal \\ 
		CO2 & CO2 conc. (ppm)\\ 
		CO2int & CO2 conc. with missing values filled \\
    Trend & Long-term trend \\ 
    Full & All days in month recorded \\ 
		\hline
\end{tabularx}

### US GNP values: *gnp.csv*

\begin{tabularx}{\linewidth}{| l | X |}
     \hline
  	Column header & Variable \\ 
		\hline
		Time & Year-Quarter as decimal \\ 
		GNP & GNP \\ 
		\hline
\end{tabularx}

### Canadian cigarette consumption data: *CanadaCigaretteConsumption.csv*

\begin{tabularx}{\linewidth}{| l | X |}
     \hline
  	Column header & Variable \\ 
		\hline
		Q & Consumption \\ 
		Y & Income \\ 
  	P & Price \\ 
		\hline
\end{tabularx}

## R code covered in lab
\begin{tabularx}{\linewidth}{| l | X |}

\hline
R Command & Purpose \\
\hline
\multicolumn{2}{|l|}{\textbf{Variogram analysis}} \\
\hline
\texttt{ts} & Convert a vector into a time series. Requires start time point and frequency of time intervals as parameters\\
\texttt{stl} & Decomposition of  time series into trend, season and autocorrelated noise components\\
\texttt{Acf} & Calculate the autocorrelation function for a time series\\
\texttt{Pacf} & Calculate the partial autocorrelation function for a time series\\
\texttt{Arima} & Build a (S)ARIMA(X) time series model. Require several parameters, notably the order of the various AR, MA and I submodels \\
\texttt{forecast} & Predictive forecast from ARIMA model, together with uncertainty estimates\\
\hline
\end{tabularx}

